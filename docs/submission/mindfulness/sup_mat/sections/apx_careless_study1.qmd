
To ensure data quality, we employed a comprehensive set of behavioral indicators to detect inattentive or insufficient-effort responding, including the compliance rate and metrics designed to identify aberrant response patterns [@ulitzsch2024screen; @hasselhorn2023modeling]. These indicators evaluated both response patterns and supplementary data (e.g., timing metrics, engagement levels), flagging anomalies that might suggest inattentive or low-effort responding [@meade2012identifying]. Common anomalies included response invariability, inconsistencies across responses, and multivariate outliers.

Notably, our data-quality assessment was conducted post hoc, rather than by embedding directed prompts (e.g., instructed-response items) in the survey. Although such items can effectively detect careless responding, they present several drawbacks [@ulitzsch2024screen]. First, adding items to detect inattentiveness can lengthen the questionnaire, paradoxically increasing the chance of respondent fatigue—particularly in studies using Ecological Momentary Assessment (EMA). Second, in repeated-measures contexts, these items may confuse participants who must maintain clarity about task expectations across multiple assessment occasions. By employing a post-hoc analysis using behavioral indicators, we minimized participant burden and preserved the integrity of the EMA design.

Compliance rate, defined as the ratio of completed assessments to the total number of expected assessments, served as a key criterion for gauging insufficient engagement. Participants failing to meet a prespecified compliance threshold were excluded from analyses to maintain the reliability of the dataset.

### Compliance Rate

Non-compliance introduces systematic missing data, which can bias statistical inference if the data are not missing at random. To mitigate this risk, participants with a compliance rate below 50% were excluded. Based on this criterion, seven participants were removed from the dataset.

### Time to Complete 

The **Time to Complete** (TTC) metric was calculated from the moment participants opened an EMA survey to the time they submitted it. Due to the right-skewed nature of survey completion times, we calculated each participant’s median TTC using log-transformed values. We then applied the lower fence of the interquartile range (IQR) rule to these log-transformed medians to identify unusually short completion times. No participant fell below this lower-fence threshold, indicating no evidence of careless responding based on survey duration.

### Careless responding on the State Self-Compassion Scale 

We assessed careless responding on the State Self-Compassion Scale items by using four metrics provided by the **careless** R package: the Longstring Index, Intra-Individual Response Variability (IRV), the Even-Odd Inconsistency Index, and Mahalanobis Distance. These metrics, designed to evaluate response patterns, offer a robust framework for identifying inattentive or inconsistent behavior. 

- **Longstring Index**: Measures the longest sequence of identical responses within a survey. High values suggest “straightlining,” a common marker of inattentiveness.
- **Intra-Individual Response Variability (IRV)**: Reflects the variability in responses across consecutive items. Low IRV may indicate lower engagement or attention.
- **Even-Odd Inconsistency Index**: Assesses internal consistency by correlating mean scores of even- and odd-numbered items. Lower correlations suggest greater inconsistency.
- **Mahalanobis Distance ($D^2$)**: Identifies multivariate outliers by quantifying deviations from an expected response pattern. High $D^2$ values indicate unusual response profiles.

We performed these analyses at two levels: person and occasion.

#### Person-Level Analysis of Behavioral Indices 

Participants were flagged as potential careless responders if their index scores exceeded the 95th percentile on any of the four metrics. Participants flagged on more than two indices were classified as careless responders.

The following R script illustrates how we counted participants who exceeded thresholds for various combinations of indices:

```r
vectors <- list(mahad_bad, longstring_bad, irv_bad, even_odd_bad)

# Count shared elements across index combinations
shared_counts <- map(2:4, ~ {
  combos <- combn(vectors, .x, simplify = FALSE)
  shared <- map(combos, ~ Reduce(intersect, .x)) %>% unlist() %>% unique()
  length(shared)
}) %>% set_names(paste0("shared_by_", 2:4))

# Results
shared_counts
# $shared_by_2
# [1] 3
# 
# $shared_by_3
# [1] 0
# 
# $shared_by_4
# [1] 0
```

No participant exceeded thresholds on more than two indices during the EMA phase, so no exclusions were made based on these metrics.

#### Occasion-Level Analysis of Behavioral Indices 

To complement the participant-level analysis, we conducted an occasion-level analysis to capture instances of momentary inattentiveness. This approach allowed us to detect subtle, transient lapses in engagement that might otherwise remain undetected in participant-level summaries.

By evaluating responses at the occasion level:

- momentary lapses in engagement could be flagged without excluding an entire participant;
- the analysis gained granularity, enabling targeted handling of problematic data points;
- the design aligns with recent work emphasizing the importance of evaluating variability over time in EMA research [@hasselhorn2023modeling].

We calculated the four careless-responding indices (Longstring, IRV, Even-Odd Inconsistency, and Mahalanobis Distance) separately for each occasion on the State Self-Compassion Scale (SCS). We then flagged occasions exceeding **adjusted fences** derived from bootstrapped IQRs. Finally, we aggregated flagged occasions to compute a proportion of flagged data points per participant.

##### Results

- **Distribution of Flagged Occasions**:
  - **80.4%** of occasions were never flagged by any metric.
  - **17.7%** of occasions were flagged on one metric, indicating mild evidence of momentary inattention.
  - **1.77%** of occasions were flagged on two metrics, suggesting stronger evidence of momentary lapses.
  - **<0.2%** of occasions were flagged on three or more metrics, representing very rare instances of substantial carelessness.

- **Proportion of Flagged Occasions per Participant**:
  - the mean proportion of occasions flagged on two or more metrics per participant was **<2%**, reflecting low overall incidence of momentary inattentiveness;
  - no participant showed persistently high flagging rates across occasions, underscoring the reliability of the dataset.


### Final Assessment

Both the person-level and occasion-level analyses confirmed a low incidence of inattentive or careless responding. These findings attest to the dataset’s robustness and highlight the strengths of this study’s design.

Compared to other study designs—particularly those involving paid participants, such as Amazon Mechanical Turk—our sample exhibited notably fewer inattentive responses. This discrepancy is likely attributable to varying levels of intrinsic motivation [@aruguete2019serious]. For instance, participants on Amazon Mechanical Turk may exhibit lower intrinsic motivation due to minimal monetary compensation, while our volunteers, who committed to completing weekly surveys over a two-month period, were likely driven by higher intrinsic motivation.

Recent research supports these observations in the context of EMA studies. For example, @hasselhorn2023modeling used multilevel latent class analysis (ML-LCA) with volunteer participants to identify profiles of momentary careless responding at the occasion level and latent classes of individuals who varied in their distribution of careless responses across occasions. They identified four latent classes: "careful," "frequently careless," and two categories of "infrequently careless" respondents. Notably, the "frequently careless" class accounted for only 2% of participants, even in a more intensive EMA design involving multiple daily notifications over seven consecutive days.

Study design plays a critical role in influencing participant compliance and data quality. Intensive EMA designs, characterized by frequent and intrusive assessments, are associated with participant fatigue, reduced response accuracy, and increased inattentiveness [@shiffman2008ecological]. In contrast, our study employed a less intensive approach, with participants receiving only one notification per week rather than multiple daily prompts.

Other additional factors likely contributed to the minimization of inattentive responses in our study:

- **Brevity of EMA questionnaires:** Participants faced minimal cognitive burden, encouraging thoughtful responses.
- **Flexibility to discontinue participation:** Participants retained the option to withdraw at any time, reducing potential disengagement due to fatigue or other external pressures.

These features collectively reduced the likelihood of inattentive responses compared to traditional cross-sectional surveys [@welling2021possible] and more intensive EMA studies [@hasselhorn2023modeling].

### Data Reanalysis

To evaluate whether excluding flagged occasions—defined as those identified on $\geq$ 2 metrics—would affect the primary outcomes, we conducted a secondary analysis of the supplementary analysis described in @sec-nomothetic-an-study1. This analysis focused on the nomothetic model assessing the relationships between the Uncompassionate Self (UCS) and Compassionate Self (CS) components of state self-compassion, after controlling for Negative Affect and Context Evaluation. Results from the full dataset are provided in Figure 1.

The reanalysis produced results that were effectively identical to those derived from the full dataset, underscoring the robustness of the findings. Hence, we retained all participants who met the compliance requirements in our final analyses. Below is a table summarizing the posterior estimates from the reanalysis, alongside their standard deviations and 89% credibility intervals:

| Variable              | Mean     | SD      | Q5.5   | Q94.5  |
|-----------------------|----------|---------|--------|--------|
| alpha_ucs            | -0.0005  | 0.0403  | -0.0644|  0.0643|
| beta_cs              | -0.4470  | 0.0183  | -0.4770| -0.4180|
| beta_negative_affect |  0.1160  | 0.0059  |  0.1070|  0.1250|
| beta_context_valence |  0.0101  | 0.0055  |  0.0013|  0.0189|

These results confirm the reliability of the final dataset and the validity of our conclusions, demonstrating a high level of data quality in this long-term EMA study design.
